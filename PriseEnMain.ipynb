{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (2803328332.py, line 140)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 140\u001b[0;36m\u001b[0m\n\u001b[0;31m    def get_action(self, state, epsilon=None,env):\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "\n",
    "# import os\n",
    "# os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = (state, action, reward, terminated, next_state)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.choices(self.memory, k=batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic neural net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        batch_size,\n",
    "        buffer_capacity,\n",
    "        update_target_every,\n",
    "        epsilon_start,\n",
    "        decrease_epsilon_factor,\n",
    "        epsilon_min,\n",
    "        learning_rate,\n",
    "    ):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.update_target_every = update_target_every\n",
    "\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.decrease_epsilon_factor = (\n",
    "            decrease_epsilon_factor  # larger -> more exploration\n",
    "        )\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        # add data to replay buffer\n",
    "        self.buffer.push(\n",
    "            torch.tensor(state).unsqueeze(0),\n",
    "            torch.tensor([[action]], dtype=torch.int64),\n",
    "            torch.tensor([reward]),\n",
    "            torch.tensor([terminated], dtype=torch.int64),\n",
    "            torch.tensor(next_state).unsqueeze(0),\n",
    "        )\n",
    "\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return np.inf\n",
    "\n",
    "        # get batch\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        (\n",
    "            state_batch,\n",
    "            action_batch,\n",
    "            reward_batch,\n",
    "            terminated_batch,\n",
    "            next_state_batch,\n",
    "        ) = tuple([torch.cat(data) for data in zip(*transitions)])\n",
    "\n",
    "        values = self.q_net.forward(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute the ideal Q values\n",
    "        with torch.no_grad():\n",
    "            next_state_values = (1 - terminated_batch) * self.target_net(\n",
    "                next_state_batch\n",
    "            ).max(1)[0]\n",
    "            targets = next_state_values * self.gamma + reward_batch\n",
    "\n",
    "        loss = self.loss_function(values, targets.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_value_(self.q_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if not ((self.n_steps + 1) % self.update_target_every):\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        self.decrease_epsilon()\n",
    "\n",
    "        self.n_steps += 1\n",
    "        if terminated:\n",
    "            self.n_eps += 1\n",
    "\n",
    "        return loss.detach().numpy()\n",
    "\n",
    "    def get_action(self, state, epsilon=None):\n",
    "        \"\"\"\n",
    "        Return action according to an epsilon-greedy exploration policy\n",
    "        \"\"\"\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.get_q(state))\n",
    "\n",
    "    def decrease_epsilon(self):\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_start - self.epsilon_min) * (\n",
    "            np.exp(-1.0 * self.n_eps / self.decrease_epsilon_factor)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (1251903171.py, line 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[26], line 62\u001b[0;36m\u001b[0m\n\u001b[0;31m    action = agent.get_action(state, epsilon=None,env)\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"highway-fast-v0\", render_mode='rgb_array')\n",
    "\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "gamma = 0.8\n",
    "batch_size = 32\n",
    "buffer_capacity = 15000\n",
    "update_target_every = 50\n",
    "epsilon_start = 0.9\n",
    "decrease_epsilon_factor = 1000\n",
    "epsilon_min = 0.05\n",
    "learning_rate = 5e-4\n",
    "\n",
    "\n",
    "arguments = (action_space,\n",
    "            observation_space,\n",
    "            gamma,\n",
    "            batch_size,\n",
    "            buffer_capacity,\n",
    "            update_target_every, \n",
    "            epsilon_start, \n",
    "            decrease_epsilon_factor, \n",
    "            epsilon_min,\n",
    "            learning_rate,\n",
    "        )\n",
    "\n",
    "N_episodes = 300\n",
    "\n",
    "agent = DQN(*arguments)\n",
    "\n",
    "def eval_agent(agent, env, n_sim=5):\n",
    "    env_copy = deepcopy(env)\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "    for i in range(n_sim):\n",
    "        state, _ = env_copy.reset()\n",
    "        reward_sum = 0\n",
    "        done = False\n",
    "        while not done: \n",
    "            action = agent.get_action(state, 0)\n",
    "            state, reward, terminated, truncated, _ = env_copy.step(action)\n",
    "            reward_sum += reward\n",
    "            done = terminated or truncated\n",
    "        episode_rewards[i] = reward_sum\n",
    "    return episode_rewards\n",
    "\n",
    "def train_1(env, agent, N_episodes, eval_every=10, reward_threshold=300):\n",
    "    total_time = 0\n",
    "    losses = []\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for ep in range(N_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        print('ep:'+str(ep))\n",
    "        i=0\n",
    "        while not done:\n",
    "            print('i:'+str(i))\n",
    "            i+=1\n",
    "            # Select action using epsilon-greedy policy\n",
    "            action = agent.get_action(state, epsilon=None)\n",
    "            print(action)\n",
    "            \n",
    "            # Take a step in the environment\n",
    "            next_state, reward, terminated, truncated, _= env.step(action)\n",
    "            \n",
    "            # Update total episode reward\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Store the transition in the replay buffer and train the agent\n",
    "            #agent.store_transition(state, action, reward, next_state, done)\n",
    "            loss_val = agent.update(state, action, reward, terminated, next_state)\n",
    "            if loss_val is not None:\n",
    "                losses.append(loss_val)\n",
    "\n",
    "            state = next_state\n",
    "            total_time += 1\n",
    "            done = terminated or truncated\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "        if (ep + 1) % eval_every == 0:\n",
    "            rewards = eval_agent(agent, env)\n",
    "            print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n",
    "            if np.mean(rewards) >= reward_threshold:\n",
    "                break\n",
    "\n",
    "    return losses\n",
    "\n",
    "def train_2(env, agent, N_episodes, eval_every=10, reward_threshold=300):\n",
    "    total_time = 0\n",
    "    state, _ = env.reset()\n",
    "    losses = []\n",
    "    for ep in range(N_episodes):\n",
    "        print(ep)\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        i=0\n",
    "        while not done: \n",
    "            i+=1\n",
    "            print(i)\n",
    "            state, _ = env.reset()\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            print(next_state)\n",
    "            print('******')\n",
    "            print(reward)\n",
    "            print('******')\n",
    "            print(terminated)\n",
    "            print('******')\n",
    "            print(truncated)\n",
    "            print('******')\n",
    "            print(_)\n",
    "\n",
    "            loss_val = agent.update(state, action, reward, terminated, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            losses.append(loss_val)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            total_time += 1\n",
    "\n",
    "        if ((ep+1)% eval_every == 0):\n",
    "            rewards = eval_agent(agent, env)\n",
    "            print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n",
    "            if np.mean(rewards) >= reward_threshold:\n",
    "                break\n",
    "                \n",
    "    return losses\n",
    "\n",
    "def reset(self):\n",
    "    hidden_size = 128\n",
    "\n",
    "    obs_size = self.observation_space.shape[0]\n",
    "    n_actions = self.action_space.n\n",
    "\n",
    "    self.buffer = ReplayBuffer(self.buffer_capacity)\n",
    "    self.q_net = Net(obs_size, hidden_size, n_actions)\n",
    "    self.target_net = Net(obs_size, hidden_size, n_actions)\n",
    "\n",
    "    self.loss_function = nn.MSELoss()\n",
    "    self.optimizer = optim.Adam(\n",
    "        params=self.q_net.parameters(), lr=self.learning_rate\n",
    "    )\n",
    "\n",
    "    self.epsilon = self.epsilon_start\n",
    "    self.n_steps = 0\n",
    "    self.n_eps = 0\n",
    "\n",
    "\n",
    "# Run the training loop\n",
    "losses = train_1(env, agent, N_episodes)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "# Evaluate the final policy\n",
    "rewards = eval_agent(agent, env, 20)\n",
    "print(\"\")\n",
    "print(\"mean reward after training = \", np.mean(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "import highway_env  # noqa: F401\n",
    "\n",
    "TRAIN = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the environment\n",
    "    env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\")\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    # Create the model\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        policy_kwargs=dict(net_arch=[256, 256]),\n",
    "        learning_rate=5e-4,\n",
    "        buffer_size=15000,\n",
    "        learning_starts=200,\n",
    "        batch_size=32,\n",
    "        gamma=0.8,\n",
    "        train_freq=1,\n",
    "        gradient_steps=1,\n",
    "        target_update_interval=50,\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"highway_dqn/\",\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    if TRAIN:\n",
    "        model.learn(total_timesteps=int(2e4))\n",
    "        model.save(\"highway_dqn/model\")\n",
    "        del model\n",
    "\n",
    "    # Run the trained model and record video\n",
    "    model = DQN.load(\"highway_dqn/model\", env=env)\n",
    "    env = RecordVideo(\n",
    "        env, video_folder=\"highway_dqn/videos\", episode_trigger=lambda e: True\n",
    "    )\n",
    "    env.unwrapped.set_record_video_wrapper(env)\n",
    "    env.configure({\"simulation_frequency\": 15})  # Higher FPS for rendering\n",
    "\n",
    "    for videos in range(10):\n",
    "        done = truncated = False\n",
    "        obs, info = env.reset()\n",
    "        while not (done or truncated):\n",
    "            # Predict\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            # Get reward\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            # Render\n",
    "            env.render()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
